{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"project.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 01 – The Other Side of Gradescope\n",
    "\n",
    "## DSC 80, Fall 2021\n",
    "\n",
    "### Checkpoint Due Date: Thursday, October 7\n",
    "### Due Date: Thursday, October 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Instructions\n",
    "\n",
    "This Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems.  \n",
    "* Like the lab, your coding work will be developed in the accompanying `project.py` file, that will be imported into the current notebook. This code will be autograded.\n",
    "* **For the checkpoint, you only need to turn in a `project.py` containing solutions for questions 1-4**\n",
    "    - The checkpoint autograder on Gradescope does not thoroughly check your code -- it only runs the doctests on problems 1-4 to make sure that you have completed them. When you submit the final version of the project, we will use more tests to check these answers more thoroughly.\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are **encouraged to write your own additional functions** to solve the questions! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `project.py` -- however, be sure to upload these to gradescope as well!\n",
    "- Always document your code!\n",
    "\n",
    "**Tips for testing the correctness of your answers!**\n",
    "Once you have your work saved in the .py file, you should import the `project` to test your function out in the notebook. In the notebook you should inspect/analyze the output to assess its correctness!\n",
    "* Run your functions on the main dataset (`grades`) and ask yourself if the output *looks correct.*\n",
    "* Run your functions on very small datasets (e.g. 1-5 row table), calculate the expected response by hand, and see if the function output matches (this *is* unit-testing your code with data).\n",
    "* Run your functions on (large and small) samples of the dataset `grades` (with and without replacement). Does your code break? Or does it still run as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Assignment\n",
    "\n",
    "The file contains the grade-book from a fictional data science course with 535 students. \n",
    "\n",
    "**Note: this dataset is synthetically generated; it does not contain real student grades. The course syllabus below is also similar, but not exactly the same as the course syllabus for this class!**\n",
    "\n",
    "In this project, you will:\n",
    "1. clean and process the data to compute total course grades according to a fictional syllabus (below),\n",
    "2. qualitatively understand how students did in the course,\n",
    "3. understand how student grades vary with small changes in performance on each assignment.\n",
    "\n",
    "---\n",
    "\n",
    "The course syllabus for this fictional class is as follows:\n",
    "\n",
    "* Lab assignments \n",
    "    - Each are worth the same amount, regardless of each lab's raw point total.\n",
    "    - The lowest lab is dropped.\n",
    "    - Each lab may be revised for one week after submission for a 10% penalty, for two weeks after submission for a 30% penalty, and beyond that for a 60% penalty. Such revisions are reflected in the `Lateness` columns in the gradebook.\n",
    "    - Labs are 20% of the total grade.\n",
    "* Projects \n",
    "    - Each project consists of an autograded portion, and *possibly* a free response portion.\n",
    "    - The total points for a single project consist of the sum of the raw score of the two portions.\n",
    "    - Each are worth the same amount, regardless of each project's raw point total.\n",
    "    - Projects are 30% of the total grade.\n",
    "* Checkpoints\n",
    "    - Project checkpoints are worth 2.5% of the total grade.\n",
    "* Discussion\n",
    "    - Discussion notebooks are worth 2.5% of the total grade.\n",
    "* Exams\n",
    "    - The midterm is worth 15% of the total grade.\n",
    "    - The final is worth 30% of the total grade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on generalization\n",
    "\n",
    "You may assume that your code will only need to work on a gradebook for a class with the syllabus given above. That is, you may assume that the dataframe `grades` looks like the given one in `data/grades.csv`.\n",
    "\n",
    "However, such a class:\n",
    "1. may have a different numbers of labs, projects, discussions, and project checkpoints.\n",
    "2. may have a different number of students.\n",
    "\n",
    "You may assume the course components and the naming conventions are as given in the data file.\n",
    "\n",
    "The dataset was generated by Gradescope; you must attempt to reason about the data as given using what you know as a student who uses Gradescope.\n",
    "\n",
    "### A note on 'putting everything together'\n",
    "\n",
    "The goal of this project is to create and assess final grades for a fictional course; if anything, the process is broken down into functions for your convenience and guidance. Here are a few remarks and tips for approaching the projects:\n",
    "1. If you are having trouble figuring out what a question is asking you to do, look at the big picture and try to understand what the current step is doing to contribute to this big picture. This may clarify what's being asked!\n",
    "1. These questions intentionally build off of each other and the final result matters! In fact, you can 'get a question correct', but only receive partial credit on it because a previous answer was wrong.\n",
    "    - Credit for a question will typically receive partial credit based on *how close* your answer is to correct (as well as some credit for a solution in the correct form). \n",
    "    - You should try to assess your answer to each question based on what you understand of the data. This might involve writing extensive code (that isn't turned in) just to check your work! Suggestions on checking your work are given in the assignment, but you should also think of your own ways of checking your work.\n",
    "    - As you do this project, think about the data from the perspective of the student (which should be easy to do!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_fp = os.path.join('data', 'grades.csv')\n",
    "grades = pd.read_csv(grades_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>College</th>\n",
       "      <th>Level</th>\n",
       "      <th>lab01</th>\n",
       "      <th>lab01 - Max Points</th>\n",
       "      <th>lab01 - Lateness (H:M:S)</th>\n",
       "      <th>lab02</th>\n",
       "      <th>lab02 - Max Points</th>\n",
       "      <th>lab02 - Lateness (H:M:S)</th>\n",
       "      <th>project01</th>\n",
       "      <th>...</th>\n",
       "      <th>discussion07 - Lateness (H:M:S)</th>\n",
       "      <th>discussion08</th>\n",
       "      <th>discussion08 - Max Points</th>\n",
       "      <th>discussion08 - Lateness (H:M:S)</th>\n",
       "      <th>discussion09</th>\n",
       "      <th>discussion09 - Max Points</th>\n",
       "      <th>discussion09 - Lateness (H:M:S)</th>\n",
       "      <th>discussion10</th>\n",
       "      <th>discussion10 - Max Points</th>\n",
       "      <th>discussion10 - Lateness (H:M:S)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A14721419</td>\n",
       "      <td>SI</td>\n",
       "      <td>JR</td>\n",
       "      <td>99.735279</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>84.990171</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>75.282632</td>\n",
       "      <td>...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>8.895294</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>780:01:28</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A14883274</td>\n",
       "      <td>TH</td>\n",
       "      <td>JR</td>\n",
       "      <td>98.829476</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>50.784231</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>52.929482</td>\n",
       "      <td>...</td>\n",
       "      <td>669:12:21</td>\n",
       "      <td>9.022407</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>9.020283</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>9.437368</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A14164800</td>\n",
       "      <td>SI</td>\n",
       "      <td>SR</td>\n",
       "      <td>86.513369</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>47.802820</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>46.122801</td>\n",
       "      <td>...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>3.030538</td>\n",
       "      <td>10</td>\n",
       "      <td>00:04:51</td>\n",
       "      <td>7.613698</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>9.624617</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A14847419</td>\n",
       "      <td>TH</td>\n",
       "      <td>JR</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>79.121806</td>\n",
       "      <td>...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>9.249126</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A14162943</td>\n",
       "      <td>SI</td>\n",
       "      <td>JR</td>\n",
       "      <td>66.506974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>33.422412</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>41.823703</td>\n",
       "      <td>...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>4.439606</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>4.485291</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>6.282712</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>A14490387</td>\n",
       "      <td>SI</td>\n",
       "      <td>JR</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>47:26:10</td>\n",
       "      <td>82.022753</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>78.936816</td>\n",
       "      <td>...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>12:08:58</td>\n",
       "      <td>9.169447</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>A14088257</td>\n",
       "      <td>SI</td>\n",
       "      <td>SO</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>87.498073</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>72.076801</td>\n",
       "      <td>...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>A14847419</td>\n",
       "      <td>WA</td>\n",
       "      <td>JR</td>\n",
       "      <td>88.656641</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>90.326041</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>66.273252</td>\n",
       "      <td>...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>9.878661</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>8.878946</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>A14513929</td>\n",
       "      <td>TH</td>\n",
       "      <td>SR</td>\n",
       "      <td>83.799719</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>85.636947</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>63.965217</td>\n",
       "      <td>...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>7.759434</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>8.655478</td>\n",
       "      <td>10</td>\n",
       "      <td>419:06:41</td>\n",
       "      <td>8.102277</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>A14365863</td>\n",
       "      <td>WA</td>\n",
       "      <td>SR</td>\n",
       "      <td>88.389331</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>73.045171</td>\n",
       "      <td>100.0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>77.572115</td>\n",
       "      <td>...</td>\n",
       "      <td>21:47:10</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>9.515494</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>535 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PID College Level       lab01  lab01 - Max Points  \\\n",
       "0    A14721419      SI    JR   99.735279               100.0   \n",
       "1    A14883274      TH    JR   98.829476               100.0   \n",
       "2    A14164800      SI    SR   86.513369               100.0   \n",
       "3    A14847419      TH    JR  100.000000               100.0   \n",
       "4    A14162943      SI    JR   66.506974               100.0   \n",
       "..         ...     ...   ...         ...                 ...   \n",
       "530  A14490387      SI    JR  100.000000               100.0   \n",
       "531  A14088257      SI    SO  100.000000               100.0   \n",
       "532  A14847419      WA    JR   88.656641               100.0   \n",
       "533  A14513929      TH    SR   83.799719               100.0   \n",
       "534  A14365863      WA    SR   88.389331               100.0   \n",
       "\n",
       "    lab01 - Lateness (H:M:S)       lab02  lab02 - Max Points  \\\n",
       "0                   00:00:00   84.990171               100.0   \n",
       "1                   00:00:00   50.784231               100.0   \n",
       "2                   00:00:00   47.802820               100.0   \n",
       "3                   00:00:00  100.000000               100.0   \n",
       "4                   00:00:00   33.422412               100.0   \n",
       "..                       ...         ...                 ...   \n",
       "530                 47:26:10   82.022753               100.0   \n",
       "531                 00:00:00   87.498073               100.0   \n",
       "532                 00:00:00   90.326041               100.0   \n",
       "533                 00:00:00   85.636947               100.0   \n",
       "534                 00:00:00   73.045171               100.0   \n",
       "\n",
       "    lab02 - Lateness (H:M:S)  project01  ...  discussion07 - Lateness (H:M:S)  \\\n",
       "0                   00:00:00  75.282632  ...                         00:00:00   \n",
       "1                   00:00:00  52.929482  ...                        669:12:21   \n",
       "2                   00:00:00  46.122801  ...                         00:00:00   \n",
       "3                   00:00:00  79.121806  ...                         00:00:00   \n",
       "4                   00:00:00  41.823703  ...                         00:00:00   \n",
       "..                       ...        ...  ...                              ...   \n",
       "530                 00:00:00  78.936816  ...                         00:00:00   \n",
       "531                 00:00:00  72.076801  ...                         00:00:00   \n",
       "532                 00:00:00  66.273252  ...                         00:00:00   \n",
       "533                 00:00:00  63.965217  ...                         00:00:00   \n",
       "534                 00:00:00  77.572115  ...                         21:47:10   \n",
       "\n",
       "    discussion08  discussion08 - Max Points  discussion08 - Lateness (H:M:S)  \\\n",
       "0       8.895294                         10                         00:00:00   \n",
       "1       9.022407                         10                         00:00:00   \n",
       "2       3.030538                         10                         00:04:51   \n",
       "3      10.000000                         10                         00:00:00   \n",
       "4       4.439606                         10                         00:00:00   \n",
       "..           ...                        ...                              ...   \n",
       "530    10.000000                         10                         12:08:58   \n",
       "531    10.000000                         10                         00:00:00   \n",
       "532     9.878661                         10                         00:00:00   \n",
       "533     7.759434                         10                         00:00:00   \n",
       "534    10.000000                         10                         00:00:00   \n",
       "\n",
       "    discussion09  discussion09 - Max Points  discussion09 - Lateness (H:M:S)  \\\n",
       "0      10.000000                         10                        780:01:28   \n",
       "1       9.020283                         10                         00:00:00   \n",
       "2       7.613698                         10                         00:00:00   \n",
       "3       9.249126                         10                         00:00:00   \n",
       "4       4.485291                         10                         00:00:00   \n",
       "..           ...                        ...                              ...   \n",
       "530     9.169447                         10                         00:00:00   \n",
       "531    10.000000                         10                         00:00:00   \n",
       "532     8.878946                         10                         00:00:00   \n",
       "533     8.655478                         10                        419:06:41   \n",
       "534     9.515494                         10                         00:00:00   \n",
       "\n",
       "    discussion10  discussion10 - Max Points  discussion10 - Lateness (H:M:S)  \n",
       "0      10.000000                         10                         00:00:00  \n",
       "1       9.437368                         10                         00:00:00  \n",
       "2       9.624617                         10                         00:00:00  \n",
       "3      10.000000                         10                         00:00:00  \n",
       "4       6.282712                         10                         00:00:00  \n",
       "..           ...                        ...                              ...  \n",
       "530    10.000000                         10                         00:00:00  \n",
       "531    10.000000                         10                         00:00:00  \n",
       "532    10.000000                         10                         00:00:00  \n",
       "533     8.102277                         10                         00:00:00  \n",
       "534    10.000000                         10                         00:00:00  \n",
       "\n",
       "[535 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started: enumerating the assignments\n",
    "\n",
    "First, you will list all the 'assignment names' and what part of the syllabus to which they belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**\n",
    "\n",
    "Create a function `get_assignment_names` that takes in a dataframe like `grades` and returns a dictionary with the following structure:\n",
    "- The keys are the general areas of the syllabus: `lab, project, midterm, final, disc, checkpoint`\n",
    "- The values are lists that contain the assignment names of that type. For example the lab assignments all have names of the form `labXX` where `XX` is a zero-padded two digit number. See the doctests for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lab': ['lab01',\n",
       "  'lab02',\n",
       "  'lab03',\n",
       "  'lab04',\n",
       "  'lab05',\n",
       "  'lab06',\n",
       "  'lab07',\n",
       "  'lab08',\n",
       "  'lab09'],\n",
       " 'project': ['project01', 'project02', 'project03', 'project04', 'project05'],\n",
       " 'midterm': ['Midterm'],\n",
       " 'final': ['Final'],\n",
       " 'disc': ['discussion01',\n",
       "  'discussion02',\n",
       "  'discussion03',\n",
       "  'discussion04',\n",
       "  'discussion05',\n",
       "  'discussion06',\n",
       "  'discussion07',\n",
       "  'discussion08',\n",
       "  'discussion09',\n",
       "  'discussion10'],\n",
       " 'checkpoint': ['project02_checkpoint01',\n",
       "  'project02_checkpoint02',\n",
       "  'project03_checkpoint01']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_assignment_names(grades):\n",
    "    \n",
    "    conv = {'lab': 'lab', 'project': 'project',\n",
    "            'Midterm': 'midterm', 'Final': 'final',\n",
    "            'discussion': 'disc', 'project_checkpoint': 'checkpoint'}\n",
    "    cols = [\"lab\", \"project\", \"midterm\", \"final\", \"disc\", \"checkpoint\"]\n",
    "    sol = {i:list() for i in cols}\n",
    "\n",
    "    for i in grades.columns.values:\n",
    "        temp = \"\".join([j for j in i if not j.isdigit()])\n",
    "        if temp in conv.keys():\n",
    "            sol[conv[temp]].append(i)\n",
    "\n",
    "    return sol\n",
    "\n",
    "get_assignment_names(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing project grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Compute the total score for the project portion of the course according to the syllabus. Create a function `projects_total` that takes in `grades` and computes the total project grade for the quarter according to the syllabus. The output Series should contain values between 0 and 1.\n",
    "\n",
    "*Note*: Don't forget to properly handle students who didn't turn in assignments! (Use your experience and common sense).\n",
    "\n",
    "*Note:* To check your work, try (1) calculating the score for a few types of students by hand, and (2) calculate the statistics for the class performance on each individual course project, making sure they look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/y8hjc_rd18g3zw5xh5l4_y8r0000gn/T/ipykernel_80350/1935500803.py:13: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return sol.mean(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      0.916234\n",
       "1      0.765932\n",
       "2      0.681279\n",
       "3      0.962581\n",
       "4      0.737446\n",
       "         ...   \n",
       "530    0.949434\n",
       "531    0.866795\n",
       "532    0.862050\n",
       "533    0.813468\n",
       "534    0.939433\n",
       "Length: 535, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def projects_total(grades):\n",
    "    temp = grades.fillna(0)    #replace null/None/nan values\n",
    "    sol = pd.DataFrame()\n",
    "    sol[\"PID\"] = grades[\"PID\"]\n",
    "    \n",
    "    for i in get_assignment_names(temp)[\"project\"]:\n",
    "        if (i + \"_free_response\") in temp.columns.values:\n",
    "            sol[i] = ((temp[i] + temp[i + \"_free_response\"]) / \n",
    "                      (temp[i + \" - Max Points\"] + temp[i + \"_free_response\" + \" - Max Points\"]))\n",
    "        else:\n",
    "            sol[i] = temp[i] / temp[i + \" - Max Points\"]\n",
    "        \n",
    "    return sol.mean(axis=1)\n",
    "\n",
    "projects_total(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing lab grades\n",
    "\n",
    "Now, you will clean and process the lab grades, which is a little more complicated. To do this, you will develop functions that:\n",
    "- 'normalize' the grades, \n",
    "- adjust for late submissions, \n",
    "- drop the lowest lab grade, and \n",
    "- creates a total lab score for each student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Unfortunately, Gradescope sometimes experiences a delay in registering when an assignment is submitted during \"periods of heavy usage\" (i.e. near a submission deadline). You need to assess when a student's assignment was actually turned in on time, even if Gradescope did not process it in time. To do this, it is helpful to know:\n",
    "* Every late submission has to be submitted by a TA (late submissions are turned off).\n",
    "* TAs never submitted a late assignment \"just after\" the deadline. \n",
    "* The deadlines were at midnight and students had to come to staff hours to late-submit their assignment.\n",
    "\n",
    "Create a function `last_minute_submissions` that takes in the dataframe `grades` and outputs the number of submissions on each *lab* assignment that were turned in on time by a student, yet marked 'late' by Gradescope. See the doctest for more details.\n",
    "\n",
    "*Note:* You have to figure out what truly is a late submission by looking at the data and understanding the facts about the data generating process above. There is some ambiguity in finding which submissions are truly late; you will *make a best guess for a threshold* by looking at this dataset. This question is about 'cleaning' a messy 'data recording process'.\n",
    "\n",
    "*Note 2:* The return value of your function should only contain counts for the *labs*; other assignment types do not need to be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def last_minute_submissions(grades):\n",
    "    labs = get_assignment_names(grades)[\"lab\"]    \n",
    "    sol = {}\n",
    "    \n",
    "    for i in labs:\n",
    "        curr = grades.columns.str.contains(i)\n",
    "        late = grades.columns[curr]\n",
    "        \n",
    "        temp = pd.DataFrame(grades, columns = late)\n",
    "        temp = temp[temp[i + \" - Lateness (H:M:S)\"] != \"00:00:00\"]\n",
    "        \n",
    "        # get only under 8 hours\n",
    "        temp = temp[(temp[i + \" - Lateness (H:M:S)\"].str.slice(stop = 2)).astype(int) <= 8]\n",
    "        sol[i] = temp.shape[0]\n",
    "        \n",
    "    return pd.Series(sol)\n",
    "\n",
    "#last_minute_submissions(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Now you need to adjust the lab grades for late submissions -- however, you need to take into account your investigation in the previous question, since students shouldn't be penalized by a bug in Gradescope!\n",
    "\n",
    "Create a function `lateness_penalty` that takes in a 'Lateness' column and returns a column of penalties (represented by the values `1.0,0.9,0.7,0.4` according to the syllabus). Only *truly* late submissions should be counted as late.\n",
    "\n",
    "*Note*: For the purpose of this project, we will only be calculating lateness for labs. There is no penalty for lateness for projects, discussions, nor checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lateness_penalty(lateness):\n",
    "    \n",
    "    # inner function to apply on entire parameter column\n",
    "    def inner_lateness(value):\n",
    "        temp = int(value.split(\":\")[0])\n",
    "        if temp == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        if temp < (24 * 7):\n",
    "            return 0.9\n",
    "        \n",
    "        if temp < (24 * 14):\n",
    "            return 0.7\n",
    "\n",
    "        return 0.4\n",
    "    \n",
    "    \n",
    "    dupl = lateness\n",
    "    dupl = dupl.apply(lambda x: \"00:00:00\" if int(x.split(\":\")[0]) <= 8 else x)\n",
    "    return dupl.apply(inner_lateness)\n",
    "\n",
    "#lateness_penalty(grades[\"lab06 - Lateness (H:M:S)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Create a function `process_labs` that takes in a dataframe like `grades` and returns a dataframe of processed lab scores. The output should:\n",
    "* share the same index as `grades`,\n",
    "* have columns given by the lab assignment names (e.g. `lab01,...lab10`)\n",
    "* have values representing the lab grades for each assignment, adjusted for Lateness and scaled to a score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab01</th>\n",
       "      <th>lab02</th>\n",
       "      <th>lab03</th>\n",
       "      <th>lab04</th>\n",
       "      <th>lab05</th>\n",
       "      <th>lab06</th>\n",
       "      <th>lab07</th>\n",
       "      <th>lab08</th>\n",
       "      <th>lab09</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997353</td>\n",
       "      <td>0.849902</td>\n",
       "      <td>0.637744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994518</td>\n",
       "      <td>0.389141</td>\n",
       "      <td>0.887917</td>\n",
       "      <td>0.874913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.988295</td>\n",
       "      <td>0.507842</td>\n",
       "      <td>0.714477</td>\n",
       "      <td>0.783672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393887</td>\n",
       "      <td>0.914061</td>\n",
       "      <td>0.944378</td>\n",
       "      <td>0.902977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.865134</td>\n",
       "      <td>0.478028</td>\n",
       "      <td>0.433667</td>\n",
       "      <td>0.738875</td>\n",
       "      <td>0.927838</td>\n",
       "      <td>0.345076</td>\n",
       "      <td>0.734070</td>\n",
       "      <td>0.718204</td>\n",
       "      <td>0.757840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925903</td>\n",
       "      <td>0.950614</td>\n",
       "      <td>0.891614</td>\n",
       "      <td>0.688403</td>\n",
       "      <td>0.985371</td>\n",
       "      <td>0.963307</td>\n",
       "      <td>0.777880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.665070</td>\n",
       "      <td>0.334224</td>\n",
       "      <td>0.706932</td>\n",
       "      <td>0.747915</td>\n",
       "      <td>0.659720</td>\n",
       "      <td>0.731345</td>\n",
       "      <td>0.607859</td>\n",
       "      <td>0.370186</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.820228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.792935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.284106</td>\n",
       "      <td>0.770281</td>\n",
       "      <td>0.931245</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874981</td>\n",
       "      <td>0.809945</td>\n",
       "      <td>0.592866</td>\n",
       "      <td>0.987597</td>\n",
       "      <td>0.759688</td>\n",
       "      <td>0.856178</td>\n",
       "      <td>0.849694</td>\n",
       "      <td>0.582645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>0.886566</td>\n",
       "      <td>0.903260</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941425</td>\n",
       "      <td>0.768909</td>\n",
       "      <td>0.967282</td>\n",
       "      <td>0.877898</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>0.837997</td>\n",
       "      <td>0.856369</td>\n",
       "      <td>0.909363</td>\n",
       "      <td>0.955287</td>\n",
       "      <td>0.737854</td>\n",
       "      <td>0.382781</td>\n",
       "      <td>0.769093</td>\n",
       "      <td>0.947450</td>\n",
       "      <td>0.867373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>0.883893</td>\n",
       "      <td>0.730452</td>\n",
       "      <td>0.944795</td>\n",
       "      <td>0.819100</td>\n",
       "      <td>0.890968</td>\n",
       "      <td>0.858277</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>0.807974</td>\n",
       "      <td>0.663745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>535 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lab01     lab02     lab03     lab04     lab05     lab06     lab07  \\\n",
       "0    0.997353  0.849902  0.637744  1.000000  1.000000  0.994518  0.389141   \n",
       "1    0.988295  0.507842  0.714477  0.783672  1.000000  0.393887  0.914061   \n",
       "2    0.865134  0.478028  0.433667  0.738875  0.927838  0.345076  0.734070   \n",
       "3    1.000000  1.000000  0.925903  0.950614  0.891614  0.688403  0.985371   \n",
       "4    0.665070  0.334224  0.706932  0.747915  0.659720  0.731345  0.607859   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "530  0.900000  0.820228  1.000000  0.792935  1.000000  0.284106  0.770281   \n",
       "531  1.000000  0.874981  0.809945  0.592866  0.987597  0.759688  0.856178   \n",
       "532  0.886566  0.903260  1.000000  1.000000  0.941425  0.768909  0.967282   \n",
       "533  0.837997  0.856369  0.909363  0.955287  0.737854  0.382781  0.769093   \n",
       "534  0.883893  0.730452  0.944795  0.819100  0.890968  0.858277  0.989410   \n",
       "\n",
       "        lab08     lab09  \n",
       "0    0.887917  0.874913  \n",
       "1    0.944378  0.902977  \n",
       "2    0.718204  0.757840  \n",
       "3    0.963307  0.777880  \n",
       "4    0.370186  1.000000  \n",
       "..        ...       ...  \n",
       "530  0.931245  1.000000  \n",
       "531  0.849694  0.582645  \n",
       "532  0.877898  1.000000  \n",
       "533  0.947450  0.867373  \n",
       "534  0.807974  0.663745  \n",
       "\n",
       "[535 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_labs(grades):\n",
    "    #temp = grades.fillna(0)\n",
    "    temp = grades\n",
    "    output = pd.DataFrame()\n",
    "    for i in get_assignment_names(temp)[\"lab\"]:\n",
    "        late = lateness_penalty(temp[i + \" - Lateness (H:M:S)\"])\n",
    "        output[i] = (temp[i] * late) / temp[i + \" - Max Points\"]\n",
    "        \n",
    "        # clipping scores under 100% and above 0%\n",
    "        output[i] = np.clip(output[i], 0.00, 1.00)\n",
    "        \n",
    "    \n",
    "    return output\n",
    "\n",
    "process_labs(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q5 results: All test cases passed!"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "Create a function `lab_total` that takes in dataframe of processed assignments (like the output of Question 5) and computes the total lab grade for each student according to the syllabus (returning a Series). Your answers should be proportions between 0 and 1. For example, if there are only 3 labs, and a student received scores of {80%,90%,100%}, then the total score would be 0.95.\n",
    "\n",
    "*Note*: Don't forget to properly handle students who didn't turn in assignments! (Use your experience and common sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.905293\n",
       "1      0.844463\n",
       "2      0.706707\n",
       "3      0.936836\n",
       "4      0.686128\n",
       "         ...   \n",
       "530    0.901836\n",
       "531    0.841369\n",
       "532    0.947054\n",
       "533    0.860098\n",
       "534    0.865609\n",
       "Length: 535, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lab_total(processed):\n",
    "    sol = processed.fillna(0)    \n",
    "    sol = (sol.sum(axis = 1) - sol.min(axis = 1)) / (len(processed.columns) - 1)\n",
    "    return sol\n",
    "\n",
    "lab_total(process_labs(grades))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q6</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q6 results: All test cases passed!"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**\n",
    "\n",
    "Finally, you need to create the final course grades. To do this, you will add up the total of each course component according to the weights given in the syllabus. \n",
    "\n",
    "* Create a function `total_points` that takes in `grades` and returns the final course grades according to the syllabus. Course grades should be proportions between zero and one.\n",
    "* Create a function `final_grades` that takes in the final course grades as above and returns a Series of letter grades given by the standard cutoffs (`A >= .90`, `.90 > B >= .80`, `.80 > C >= .70`, `.70 > D >= .60`, `.60 > F`). You should not use rounding to determining the letter grades.\n",
    "* Create a function `letter_proportions` which takes in the dataframe `grades` and outputs a Series that contains the proportion of the class that received each grade. (This question requires you to put everything together).\n",
    "* The indices should be ordered by the proportion of the class that receives that grade, from largest to smallest.\n",
    "\n",
    "*Note 1*: Don't repeat yourself when computing the checkpoint and discussion portions of the course.\n",
    "\n",
    "*Note 2*: Only the lab portion of the course accounts for late assignments; you may assume all assignments in other portions are turned in without penalty.\n",
    "\n",
    "*Note 3*: These values should add up to exactly 1.0. If you are getting something close such as 0.99999, that means there is a slight issue with your code from above. \n",
    "\n",
    "To check your work, verify the course grade distribution and relevant statistics! Do the work by hand for a few students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def total_points(grades):\n",
    "    temp = grades.fillna(0)\n",
    "    sol = pd.DataFrame()\n",
    "    \n",
    "    sol[\"project\"] = projects_total(temp)\n",
    "    sol[\"lab\"] = lab_total(process_labs(temp))\n",
    "    \n",
    "    cols = get_assignment_names(temp)\n",
    "    rem = [\"final\", \"midterm\", \"disc\", \"checkpoint\"]\n",
    "    \n",
    "    for i in rem:\n",
    "        value = cols[i]\n",
    "        max_vals = [x + \" - Max Points\" for x in value]\n",
    "        sol[i] = temp[value].sum(axis = 1) / temp[max_vals].sum(axis = 1)\n",
    "        \n",
    "    final_grade = (sol[\"lab\"] * 0.2) + (sol[\"project\"] * 0.3) + (sol[\"checkpoint\"] * 0.025) +\\\n",
    "    (sol[\"disc\"] * 0.025) + (sol[\"midterm\"] * 0.15) + (sol[\"final\"] * 0.3)\n",
    "    return final_grade\n",
    "        \n",
    "        \n",
    "#total_points(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_grades(total):\n",
    "    def cutoff(val):\n",
    "        if val >= 0.9:\n",
    "            return \"A\"\n",
    "        elif val >= 0.8:\n",
    "            return \"B\"\n",
    "        elif val >= 0.7:\n",
    "            return \"C\"\n",
    "        elif val >= 0.6:\n",
    "            return \"D\"\n",
    "        else:\n",
    "            return \"F\"\n",
    "    \n",
    "    # use for checking values\n",
    "    #for i in range(total.size):\n",
    "        #print(\"score: \" + str(total[i]) + \" and the grade is \" + cutoff(total[i]))\n",
    "        \n",
    "    sol = total.apply(cutoff)\n",
    "    return sol\n",
    "\n",
    "#final_grades(total_points(grades))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def letter_proportions(grades):\n",
    "    temp = total_points(grades)\n",
    "    temp = final_grades(temp)\n",
    "    return temp.value_counts(normalize = True)\n",
    "\n",
    "#letter_proportions(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q7</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q7 results: All test cases passed!"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Seniors get worse grades?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "You notice that students who are seniors on average did worse in the class (if you can't verify this, you should go back and check your work!). Is this difference significant, or just due to noise?\n",
    "\n",
    "Perform a hypothesis test, assessing the likelihood of the above statement under the null hypothesis: \n",
    "> \"Seniors earn grades that are roughly equal on average to the rest of the class.\"\n",
    "\n",
    "\n",
    "Create a function `simulate_pval` which takes in the number of simulations `N` and `grades` and returns the the likelihood that the grade of seniors was worse than the average of the class as a whole under the null hypothesis (i.e. calculate the p-value).\n",
    "\n",
    "*Note:* To check your work, plot the sampling distribution and the observation. Do these values look reasonable?\n",
    "\n",
    "*Note 2*: If you sample the data, make sure you sample *with* replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulate_pval(grades, N):\n",
    "    df = pd.concat([grades[\"Level\"], pd.Series(total_points(grades), name = \"Total\")], axis = 1)\n",
    "    senior_grades = df[df[\"Level\"] == \"SR\"].get(\"Total\").mean()\n",
    "    #check with others\n",
    "    #change the column name from 0 to smth else\n",
    "    \n",
    "    avg = list()\n",
    "    for i in np.arange(N):\n",
    "        temp_sample = df[\"Total\"].sample((df[df[\"Level\"] == \"SR\"]).shape[0], replace = True)\n",
    "        temp = np.mean(temp_sample)\n",
    "        avg.append(temp)\n",
    "        \n",
    "    return (senior_grades >= pd.Series(avg)).mean()\n",
    "    \n",
    "\n",
    "#simulate_pval(grades, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q8</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q8 results: All test cases passed!"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the true distribution of grades?\n",
    "\n",
    "The gradebook for this class only reflects one particular instance of each student's performance, subject to the effects of all the little events and hiccups that occurred throughout the quarter. Might you have done better on the midterm had your roommate kept you up all night with their coughing? Wasn't it lucky that the example you were studying just before the final happened to appear on the exam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**\n",
    "\n",
    "This question will simulate these '(un)lucky, random events' by adding or subtracting random amounts to each assignment before calculating the final grades. These 'random amounts' will be drawn from a Gaussian distribution of mean 0 and a std deviation 0.02:\n",
    "```\n",
    "np.random.normal(0, 0.02, size=(num_rows, num_cols))\n",
    "```\n",
    "Intuitively, such a model says that random events may bump up or down a given grade (given as a proportion):\n",
    "- which on average has no effect on the class as a whole (mean 0),\n",
    "- which not uncommonly might perturb a grade by 2% (std dev 0.02).\n",
    "\n",
    "Create a function `total_points_with_noise` that takes in a dataframe like `grades`, adds noise to the assignments as described above, and returns the final scores using *the same procedure* as questions 1-7.\n",
    "\n",
    "*Note:* You should be able to reuse (or minorly change) the code from previous problems. Try to be DRY (don't repeat yourself)!\n",
    "\n",
    "*Note 1:* Once adding the noise to the assignment scores, use the `np.clip` function to be sure each assignment retains a score between 0% and 100%.\n",
    "\n",
    "*Note 2:* To check your work -- what would you expect the difference between the actual scores and noisy scores to be, on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/y8hjc_rd18g3zw5xh5l4_y8r0000gn/T/ipykernel_80350/1935500803.py:13: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return sol.mean(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      0.902465\n",
       "1      0.816654\n",
       "2      0.759665\n",
       "3      0.908073\n",
       "4      0.675083\n",
       "         ...   \n",
       "530    0.865660\n",
       "531    0.764832\n",
       "532    0.859648\n",
       "533    0.866322\n",
       "534    0.894719\n",
       "Length: 535, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_points(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/y8hjc_rd18g3zw5xh5l4_y8r0000gn/T/ipykernel_80350/1935500803.py:13: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return sol.mean(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      0.898010\n",
       "1      0.808848\n",
       "2      0.767833\n",
       "3      0.919537\n",
       "4      0.678776\n",
       "         ...   \n",
       "530    0.851588\n",
       "531    0.751379\n",
       "532    0.863709\n",
       "533    0.869327\n",
       "534    0.902596\n",
       "Length: 535, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def total_points_with_noise(grades):\n",
    "    #using names, we iterate through EACH assignment column in grades\n",
    "    #for each student's individual assignment, i.e. a cell in the df, \n",
    "    #val += np.random.normal(0, 0.02, grades[ass name].shape[0])\n",
    "    #then at the end we call total_points with this edited df\n",
    "    \n",
    "    temp = grades\n",
    "    names = [j for i in get_assignment_names(grades).values() for j in i]\n",
    "    \n",
    "    for i in names:\n",
    "        noise = np.random.normal(0, 0.02, len(temp[i]))\n",
    "        temp[i] += (noise * temp[i + \" - Max Points\"])\n",
    "        \n",
    "        #SOMEHOW THIS WORKS\n",
    "        #temp[i].apply(lambda x: x + np.random.normal(0, 0.02, len(grades[i])))\n",
    "        \n",
    "        #INCORRECT ANSWERS BELOW\n",
    "        #temp[i] += np.random.normal(0, 0.02)\n",
    "        #temp[i].apply(lambda x: x + np.random.normal(0, 0.02))\n",
    "\n",
    "        \n",
    "        #the third parameter makes all the noise values unique, keep it!\n",
    "        #temp[i] = np.clip(temp[i], 0.00, 100)\n",
    "        temp[i] = np.clip(temp[i], 0.00, temp[i + \" - Max Points\"].iloc[0])\n",
    "                \n",
    "    return total_points(temp)\n",
    "    \n",
    "total_points_with_noise(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q9</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q9 results: All test cases passed!"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-answer questions (hard-coded)\n",
    "\n",
    "Use your functions from above to understanding the data and answer the following questions. The function below should return **hard-coded values**. It should not compute anything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**\n",
    "\n",
    "Create a function `short_answer` with zero parameters that returns (hard-coded) answers to the following question in a list of length 5:\n",
    "\n",
    "1. For the class on average, what is the difference between students' scores (`total_points`) and their scores with noise (`total_points_with_noise`)? (Remark: plot the distribution of differences; does this align with what you know about binomial distributions?)\n",
    "2. What **percentage** of the class only sees their grade change at most (but not including) $\\pm 0.01$? (Your answer should be a number between 0 and 100.)\n",
    "3. What is the 95% confidence interval for the statistic above? (see [DSC10](https://www.inferentialthinking.com/chapters/13/3/Confidence_Intervals.html) and use `np.percentile`)\n",
    "4. What **proportion** of the class sees a change in their letter grade? (Your answer should be a number between 0 and 1.)\n",
    "5. Is the assumption behind the model in Question 9 that:\n",
    "    - The (observed) gradebook well represents the true population of students? (True or False)\n",
    "    - The noisy scores does not represent other possible observations drawn from the true population of students. (True or False)\n",
    "    - Answer `True` or `False` in a list, like `[True, True]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/y8hjc_rd18g3zw5xh5l4_y8r0000gn/T/ipykernel_80350/1935500803.py:13: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return sol.mean(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = (abs(total_points(grades) - total_points_with_noise(grades)) > 0.01).sum()\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/y8hjc_rd18g3zw5xh5l4_y8r0000gn/T/ipykernel_80350/1935500803.py:13: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return sol.mean(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0008632870417972022,\n",
       " 0.8149532710280374,\n",
       " [81.39719626168224, 87.57476635514018],\n",
       " 0.04672897196261682,\n",
       " [True, True]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def short_answer():\n",
    "    # CHECK NUMBER 9, make sure its 100% correct\n",
    "    \n",
    "    diff = total_points(grades) - total_points_with_noise(grades)\n",
    "    q1 = (total_points(grades) - total_points_with_noise(grades)).mean()\n",
    "    \n",
    "    q2 = abs(diff)\n",
    "    q2 = (q2 < 0.01).sum() / len(q2)\n",
    "\n",
    "    \n",
    "    vals = np.array([])\n",
    "    prev = total_points(grades)\n",
    "    new_gr = total_points_with_noise(grades)\n",
    "    for i in range(100):\n",
    "        prev_strap = prev.sample(prev.shape[0], replace = True)\n",
    "        new_strap = new_gr.sample(new_gr.shape[0], replace = True)\n",
    "        mic = abs(total_points(grades) - total_points_with_noise(grades))\n",
    "        per = (np.count_nonzero(mic < 0.01) / mic.shape[0]) * 100\n",
    "        vals = np.append(vals, per)\n",
    "        \n",
    "    q3 = [np.percentile(vals, 2.5), np.percentile(vals, 97.5)]\n",
    "    \n",
    "    \n",
    "    prev = final_grades(total_points(grades))\n",
    "    new_gr = final_grades(total_points_with_noise(grades))\n",
    "    q4 = np.sum(prev != new_gr) / prev.shape[0]\n",
    "    #q4 = q4.apply(ord) - final_grades(total_points(grades)).apply(ord)\n",
    "    #q4 = q4.where(q4 != 0).shape[0] / q4.shape[0]\n",
    "\n",
    "    \n",
    "    #print(\"Mean is \" + str(total_points_with_noise(grades).mean()))\n",
    "    #print(\"Min is \" + str(total_points_with_noise(grades).min()))\n",
    "    #print(\"Max is \" + str(total_points_with_noise(grades).max()))\n",
    "    #print(\"Median is \" + str(total_points_with_noise(grades).median()))\n",
    "    \n",
    "    \n",
    "    q5 = list() \n",
    "    q5.append(True) #Because the min/max/median/mean is realistic, and median/mean are around 83%\n",
    "    q5.append(True) \n",
    "    #Thinking about other possible factors outside of noise such as student getting covid \n",
    "    #and 2 weeks worth of assignments/labs/projects/etc are low\n",
    "    \n",
    "    \n",
    "    return [q1, q2, q3, q4, q5]\n",
    "\n",
    "short_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q10</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q10 results: All test cases passed!"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you finished the project!\n",
    "\n",
    "### Before you submit:\n",
    "* Be sure you run the doctests on all your code in `project.py`\n",
    "\n",
    "### To submit:\n",
    "* **Upload the .py file to gradescope**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q1 results: All test cases passed!\n",
       "\n",
       "q10 results: All test cases passed!\n",
       "\n",
       "q2 results:\n",
       "    q2 - 1 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q2 - 2 result:\n",
       "        Trying:\n",
       "            out = projects_total(grades)\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            0.7 < out.mean() < 0.9\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 2, in q2 1\n",
       "        Failed example:\n",
       "            0.7 < out.mean() < 0.9\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            False\n",
       "\n",
       "q3 results: All test cases passed!\n",
       "\n",
       "q4 results: All test cases passed!\n",
       "\n",
       "q5 results:\n",
       "    q5 - 1 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q5 - 2 result:\n",
       "        Trying:\n",
       "            out = process_labs(grades)\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            np.all((0.65 <= out.mean()) & (out.mean() <= 0.90))\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 2, in q5 1\n",
       "        Failed example:\n",
       "            np.all((0.65 <= out.mean()) & (out.mean() <= 0.90))\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            False\n",
       "\n",
       "q6 results: All test cases passed!\n",
       "\n",
       "q7 results:\n",
       "    q7 - 1 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q7 - 2 result:\n",
       "        Trying:\n",
       "            out = total_points(grades)\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            0.7 < out.mean() < 0.9\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 2, in q7 1\n",
       "        Failed example:\n",
       "            0.7 < out.mean() < 0.9\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            False\n",
       "\n",
       "    q7 - 3 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q7 - 4 result:\n",
       "        Trying:\n",
       "            out = letter_proportions(grades)\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            np.all(out.index == ['B', 'C', 'A', 'D', 'F'])\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 2, in q7 3\n",
       "        Failed example:\n",
       "            np.all(out.index == ['B', 'C', 'A', 'D', 'F'])\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"/opt/homebrew/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9/doctest.py\", line 1336, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q7 3[1]>\", line 1, in <module>\n",
       "                np.all(out.index == ['B', 'C', 'A', 'D', 'F'])\n",
       "              File \"/opt/homebrew/lib/python3.9/site-packages/pandas/core/ops/common.py\", line 69, in new_method\n",
       "                return method(self, other)\n",
       "              File \"/opt/homebrew/lib/python3.9/site-packages/pandas/core/arraylike.py\", line 32, in __eq__\n",
       "                return self._cmp_method(other, operator.eq)\n",
       "              File \"/opt/homebrew/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6058, in _cmp_method\n",
       "                result = ops.comp_method_OBJECT_ARRAY(op, self._values, other)\n",
       "              File \"/opt/homebrew/lib/python3.9/site-packages/pandas/core/ops/array_ops.py\", line 70, in comp_method_OBJECT_ARRAY\n",
       "                raise ValueError(\"Shapes must match\", x.shape, y.shape)\n",
       "            ValueError: ('Shapes must match', (4,), (5,))\n",
       "\n",
       "    q7 - 5 result:\n",
       "        Test case passed!\n",
       "\n",
       "q8 results: All test cases passed!\n",
       "\n",
       "q9 results:\n",
       "    q9 - 1 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q9 - 2 result:\n",
       "        Trying:\n",
       "            out = total_points_with_noise(grades)\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            0.7 < out.mean() < 0.9\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 2, in q9 1\n",
       "        Failed example:\n",
       "            0.7 < out.mean() < 0.9\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = get_assignment_names(grades)\n>>> out['final'] == ['Final']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = get_assignment_names(grades)\n>>> set(out.keys()) == {'lab', 'project', 'midterm', 'final', 'disc', 'checkpoint'}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = get_assignment_names(grades)\n>>> 'project02' in out['project']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q10": {
     "name": "q10",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = short_answer()\n>>> len(out) == 5\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = short_answer()\n>>> (len(out[2])) == 2 and (50 < out[2][0] < 100) and (0 < out[3] < 1)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = short_answer()\n>>> isinstance(out[4][0], bool) and isinstance(out[4][1], bool)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = projects_total(grades)\n>>> np.all((0 <= out) & (out <= 1))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = projects_total(grades)\n>>> 0.7 < out.mean() < 0.9\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = last_minute_submissions(grades)\n>>> isinstance(out, pd.Series)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = last_minute_submissions(grades)\n>>> np.all(out.index == ['lab0%d' % d for d in range(1,10)])\nTrue",
         "failure_message": "Checking that all labs, and only labs, are included in the results of last_minute_submissions.",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = last_minute_submissions(grades)\n>>> (out > 0).sum() == 8\nTrue",
         "failure_message": "Checking that there are exactly 8 labs that are incorrectly marked as late.",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = lateness_penalty(grades['lab01 - Lateness (H:M:S)'])\n>>> isinstance(out, pd.Series)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = lateness_penalty(grades['lab01 - Lateness (H:M:S)'])\n>>> set(out.unique()) <= {1.0, 0.9, 0.7, 0.4}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = process_labs(grades)\n>>> out.columns.tolist() == ['lab%02d' % x for x in range(1,10)]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = process_labs(grades)\n>>> np.all((0.65 <= out.mean()) & (out.mean() <= 0.90))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> cols = 'lab01 lab02 lab03'.split()\n>>> processed = pd.DataFrame([[0.2, 0.90, 1.0]], index=[0], columns=cols)\n>>> np.isclose(lab_total(processed), 0.95).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = total_points(grades)\n>>> np.all((0 <= out) & (out <= 1))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = total_points(grades)\n>>> 0.7 < out.mean() < 0.9\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = final_grades(pd.Series([0.92, 0.81, 0.41]))\n>>> np.all(out == ['A', 'B', 'F'])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = letter_proportions(grades)\n>>> np.all(out.index == ['B', 'C', 'A', 'D', 'F'])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = letter_proportions(grades)\n>>> out.sum() == 1.0\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = simulate_pval(grades, 1000)\n>>> 0 <= out <= 0.1\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9": {
     "name": "q9",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = total_points_with_noise(grades)\n>>> np.all((0 <= out) & (out <= 1))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = total_points_with_noise(grades)\n>>> 0.7 < out.mean() < 0.9\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
